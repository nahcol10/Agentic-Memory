import os
import chromadb
import uuid
from typing import Tuple
from dotenv import load_dotenv
from mistralai import Mistral
from chromadb.utils import embedding_functions
from .load_config import LoadConfig
from .prepare_system_prompt import prepare_system_prompt_for_rag_chatbot

load_dotenv()

class VectorDBManager:
  def __init__(self, config: LoadConfig):
    """
    Initializes the VectorDBManager

    :params config: LoadConfig instance for configuration

    """
    self.cfg = config
    self.embedding_functions = embedding_functions.MistralEmbeddingFunction(
      model=self.cfg.embedding_model
    )
    self.db_client = chromadb.PersistentClient(path=str(self.cfg.vectordb_dir))
    self.db_collection = self.db_client.get_or_create_collection(
      name=self.cfg.collection_name,
      embedding_function=self.embedding_functions,  # type: ignore
      metadata={"hnsw:space": "cosine"}
    )
    self.client = Mistral(api_key=os.getenv("MISTRAL_API_KEY"))
    self.system_prompt = prepare_system_prompt_for_rag_chatbot()


  def update_vector_db(self, msg_pairs: dict) -> None:
    """
    Update the vectordb with new message pairs 

    :params msg_pairs: A dictionary containing message pair to be added to  the database

    :return : None
    """
    self.db_collection.add(
      ids=str(uuid.uuid4()),
      documents=[str(msg_pairs)]
    )
    print("Vectordb updated.")
    return None
  
  def search_vector_db(self, query: str) -> Tuple[str, str]:
    """
    Search the vectorDB containing the chat history of user and chatbot and return the result

    :params query: The query to be used for search

    :return : The result generated by language model based on the search results.

    """
    try:
      print("Performing vector search...")
      results = self.db_collection.query(
        query_texts=[query],
        n_results=self.cfg.k
      )
      if results and "documents" in results and results["documents"]:
        documents = results["documents"][0]
        llm_result = self.prepare_search_result(documents, query)
        print("Vector Search Completed.")
        print(f"Query: {query}")
        print(f"Results: {documents}")
        print(f"LLM Result: {llm_result}")
        return "Function call successful.", llm_result
      else:
        return "Function call failed.", "No results found in vector database."
    except Exception as e:
      return "Function call failed.", f"Error: {e}"

  def prepare_search_result(self, search_result: list, query: str) -> str:
    """
    Prepare a structure search result using language model.
    :param search_result: list of search results from vector database
    :param query: The original user query.
    :return: The response generated by language model.
    """
    input = f"""
    ## Search Results: \n
    {search_result}
    
    ## Query: \n
    {query}
    """
    response = self.client.chat.complete(
      model=self.cfg.rag_model,
      messages=[
        {"role": "system", "content": self.system_prompt},
        {"role": "user", "content": input}
      ]
    )
    content = response.choices[0].message.content
    return str(content) if content else ""

  def refresh_vector_db_client(self):
    """
    Refresh the vector database client connection.
    """
    self.db_client = chromadb.PersistentClient(
      path=str(self.cfg.vectordb_dir)
    )
    self.db_collection = self.db_client.get_or_create_collection(
      name=self.cfg.collection_name,
      embedding_function=self.embedding_functions,  # type: ignore
      metadata={"hnsw:space": "cosine"}
    )

